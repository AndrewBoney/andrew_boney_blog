{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1b3fdd4a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LeJEPA Implemetation\"\n",
    "author: \"Andrew Boney\"\n",
    "date: \"2026-01-16\"\n",
    "categories: [research-paper, JEPA]\n",
    "description: \"Implementation of LeJEPA for text\"\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5d59d",
   "metadata": {},
   "source": [
    "<!--\r\n",
    "I'm going to use both prompts and my own code / note cells to form a blog in this notebook. In order for the blog to render properly I need you to avoid using \"#\" headers in your responses.\r\n",
    "-->  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93446a2",
   "metadata": {
    "time_run": "2026-02-11T09:58:59.152238+00:00"
   },
   "source": [
    "# LeJEPA Implementation for Text\n",
    "\n",
    "As a follow up to my [previous blog post](https://andrewboney.github.io/andrew_boney_blog/posts/LeJEPA/), I want to implement a LeJEPA architecture adapted for text. The original [LeJEPA paper](https://arxiv.org/abs/2511.08544) focuses on vision tasks, using a latent prediction approach where the model learns to predict representations of masked image patches from visible context.\n",
    "\n",
    "In this post, I'll adapt these principles to text by implementing a transformer-based encoder that predicts latent representations of masked text spans. The key idea is to use **span masking** - masking contiguous chunks of text rather than random tokens - which encourages the model to learn higher-level semantic representations.\n",
    "\n",
    "#### Approach\n",
    "\n",
    "I'll implement a LeJEPA-style architecture with:\n",
    "\n",
    "- An **encoder** that processes visible text tokens. \n",
    "- A **predictor** that learns to predict the target encoder's representations of masked spans.\n",
    "\n",
    "The goal is to train this model from scratch on a text dataset and evaluate how well the learned representations capture semantic information. This is primarily an implementation exercise to better understand how the LeJEPA architecture works and how it can be adapted beyond vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a86a6",
   "metadata": {},
   "source": [
    "![](../../assets/le_jepa_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59ece7",
   "metadata": {},
   "source": [
    "For this we'll need to create functions and classes for:\n",
    "- A `LeJEPAText` class that defines the LeJEPA architecture.\n",
    "- A `LeJEPATextDataModule` class that loads and preprocesses the text data.\n",
    "\n",
    "And use:\n",
    "- A text dataset, for which I'll use a standard dataset from HuggingFace.\n",
    "- A tokenizer, for which I'll use a standard open source tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a2a34",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc637be",
   "metadata": {},
   "source": [
    "## Investigating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50fa44",
   "metadata": {},
   "source": [
    "### prompt / response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebd29e",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "What would be a good dataset to use for this? Please search the web\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "For a text LeJEPA self-supervised learning experiment, here are some good dataset options:\n",
    "\n",
    "**Best for this experiment:**\n",
    "\n",
    "1. **FineWeb / FineWeb-Edu** - [*](https://huggingface.co/posts/Kseniase/676807985484672 \"@Kseniase on Hugging Face: \\\"10 Free Comprehensive Datasets for Supervised Fine-Tuning High-qualityâ€¦\\\"\") Includes 15T tokens of cleaned and deduplicated English web data. [*](https://www.analyticsvidhya.com/blog/2025/12/most-downloaded-hugging-face-datasets/ \"10 Most Downloaded Hugging Face Datasets and Their Use-cases\") The dataset focuses on cleaner, more informative content that actually helps models learn language patterns, reasoning, and structure. That is why it has become a popular choice for pretraining and fine-tuning large language models.\n",
    "\n",
    "2. **WikiText-103** - A classic choice for self-supervised text experiments. Clean Wikipedia articles with ~100M tokens - manageable size for experimentation.\n",
    "\n",
    "3. **OpenWebText** - Recreated WebText corpus, good for general text pretraining.\n",
    "\n",
    "**My recommendation for your experiment:**\n",
    "\n",
    "For a blog post / learning exercise, I'd suggest **WikiText-103** (`wikitext-103-raw-v1`) because:\n",
    "- Manageable size (~100M tokens)\n",
    "- Clean, well-structured text\n",
    "- Fast to download and iterate with\n",
    "- Standard benchmark for language modeling\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Quick to download, good for experimentation\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "```\n",
    "\n",
    "If you want something larger later, you can use `HuggingFaceFW/fineweb-edu` with streaming:\n",
    "\n",
    "```python\n",
    "# For larger scale (streaming to avoid downloading all 15T tokens)\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", split=\"train\", streaming=True)\n",
    "```\n",
    "\n",
    "Would you like me to help set up the DataModule for WikiText-103?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286174cf",
   "metadata": {},
   "source": [
    "### investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172aee9d",
   "metadata": {
    "time_run": "2026-01-20T19:30:23.290298+00:00"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2e018",
   "metadata": {
    "time_run": "2026-01-20T19:30:27.260295+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' = Valkyria Chronicles III = \\n',\n",
       "  '',\n",
       "  ' SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       "  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188bf4b8",
   "metadata": {},
   "source": [
    "### prompt / response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e5203",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Hmmm... it looks like in this dataset the text is split between documents. How can I deal with this in a dataset for training a text JEPA? \n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Great observation! The standard approach is to **concatenate all documents** then **chunk into fixed-length sequences**. This avoids short sequences and wasted padding.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128):\n",
    "        # Concatenate all non-empty text\n",
    "        all_text = \" \".join([t for t in dataset[\"text\"] if t.strip()])\n",
    "        \n",
    "        # Tokenize everything at once\n",
    "        tokens = tokenizer(all_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].squeeze()\n",
    "        \n",
    "        # Chunk into fixed-length sequences (drop remainder)\n",
    "        num_chunks = len(tokens) // max_length\n",
    "        self.chunks = tokens[:num_chunks * max_length].view(num_chunks, max_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.chunks[idx]}\n",
    "\n",
    "# Example usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "train_dataset = ChunkedTextDataset(dataset[\"train\"], tokenizer, max_length=128)\n",
    "\n",
    "print(f\"Number of chunks: {len(train_dataset)}\")\n",
    "print(f\"Sample shape: {train_dataset[0]['input_ids'].shape}\")\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- **Concatenate**: Join all documents with spaces (ignoring empty rows)\n",
    "- **Tokenize once**: More efficient than per-document tokenization\n",
    "- **Fixed chunks**: Every sample is exactly `max_length` tokens - no padding needed\n",
    "- **Drop remainder**: Discard the last partial chunk for simplicity\n",
    "\n",
    "This is the same approach used by most LLM pretraining pipelines. You can also add document separator tokens if you want the model to learn document boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58546",
   "metadata": {},
   "source": [
    "### investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefe165",
   "metadata": {
    "time_run": "2026-01-20T19:30:27.266684+00:00"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128):\n",
    "        # Concatenate all non-empty text\n",
    "        all_text = \" \".join([t for t in tqdm(dataset[\"text\"]) if t.strip()])\n",
    "        \n",
    "        # Tokenize everything at once\n",
    "        tokens = tokenizer(all_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].squeeze()\n",
    "        \n",
    "        # Chunk into fixed-length sequences (drop remainder)\n",
    "        num_chunks = len(tokens) // max_length\n",
    "        self.chunks = tokens[:num_chunks * max_length].view(num_chunks, max_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.chunks[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd98ee",
   "metadata": {
    "time_run": "2026-01-20T19:30:30.599731+00:00"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c2d23",
   "metadata": {
    "time_run": "2026-01-20T19:30:30.911390+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/36718 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|â–ˆâ–ˆâ–ˆ       | 11260/36718 [00:00<00:00, 112588.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22673/36718 [00:00<00:00, 113488.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34085/36718 [00:00<00:00, 113774.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36718/36718 [00:00<00:00, 113158.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all non-empty text\n",
    "all_text = \" \".join([t for t in tqdm(dataset[\"train\"][\"text\"]) if t.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c8514",
   "metadata": {
    "time_run": "2026-01-20T19:30:31.258507+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (219671 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize everything at once\n",
    "tokens = tokenizer(all_text[:1000000], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9e9a9",
   "metadata": {
    "time_run": "2026-01-20T19:30:31.867732+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10916756, 219671)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text), len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150864c1",
   "metadata": {},
   "source": [
    "### prompt / response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877961e9",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Can you change the tokenizer above to run a batched way? right now it appears to be crashing on the whole dataset\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128, batch_size=10000):\n",
    "        # Filter non-empty text\n",
    "        texts = [t for t in dataset[\"text\"] if t.strip()]\n",
    "        \n",
    "        # Tokenize in batches\n",
    "        all_tokens = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            encoded = tokenizer(batch, add_special_tokens=False)[\"input_ids\"]\n",
    "            for ids in encoded:\n",
    "                all_tokens.extend(ids)\n",
    "        \n",
    "        tokens = torch.tensor(all_tokens)\n",
    "        \n",
    "        # Chunk into fixed-length sequences\n",
    "        num_chunks = len(tokens) // max_length\n",
    "        self.chunks = tokens[:num_chunks * max_length].view(num_chunks, max_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.chunks[idx]}\n",
    "\n",
    "# Usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "train_dataset = ChunkedTextDataset(dataset[\"train\"], tokenizer, max_length=128)\n",
    "print(f\"Number of chunks: {len(train_dataset)}\")\n",
    "```\n",
    "\n",
    "Key change: Instead of joining all text into one giant string, we tokenize documents in batches of 10,000 and concatenate the token IDs. This avoids the memory spike from both the huge concatenated string and the single tokenizer call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00ff67",
   "metadata": {},
   "source": [
    "### investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae620f",
   "metadata": {
    "time_run": "2026-01-20T19:30:31.874712+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 9343\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128, batch_size=10000):\n",
    "        # Filter non-empty text\n",
    "        texts = [t for t in dataset[\"text\"] if t.strip()]\n",
    "        \n",
    "        # Tokenize in batches\n",
    "        all_tokens = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            encoded = tokenizer(batch, add_special_tokens=False)[\"input_ids\"]\n",
    "            for ids in encoded:\n",
    "                all_tokens.extend(ids)\n",
    "        \n",
    "        tokens = torch.tensor(all_tokens)\n",
    "        \n",
    "        # Chunk into fixed-length sequences\n",
    "        num_chunks = len(tokens) // max_length\n",
    "        self.chunks = tokens[:num_chunks * max_length].view(num_chunks, max_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunks[idx]\n",
    "\n",
    "# Usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "train_dataset = ChunkedTextDataset(dataset[\"train\"], tokenizer, max_length=256)\n",
    "print(f\"Number of chunks: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda02cbb",
   "metadata": {
    "time_run": "2026-01-20T19:30:33.620570+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198,  2311,\n",
       "           73, 13090,   645,   569, 18354,  7496,   513,  1058,   791, 47398,\n",
       "        17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,   112,\n",
       "         5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,  6578,\n",
       "          764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,   837,\n",
       "         8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,  2354,\n",
       "         2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,  2712,\n",
       "         2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,   329,\n",
       "          262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,  2869,\n",
       "          837,   340,   318,   262,  2368,   983,   287,   262,   569, 18354,\n",
       "         7496,  2168,   764, 12645,   278,   262,   976, 21748,   286, 16106,\n",
       "          290,  1103,  2488,    12,    31,   640, 11327,   355,   663, 27677,\n",
       "          837,   262,  1621,  4539, 10730,   284,   262,   717,   983,   290,\n",
       "         5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,  2422,\n",
       "         4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,  5498,\n",
       "         1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,   389,\n",
       "        46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,    88,\n",
       "        12552,   366,   764,   220,   198,   383,   983,  2540,  2478,   287,\n",
       "         3050,   837,  6872,   625,   257,  1588,  6903,   286,   262,   670,\n",
       "         1760,   319,   569, 18354,  7496, 17740,  2873,   764,  2893,   340,\n",
       "        17383,   262,  3210,  3033,   286,   262,  2168,   837,   340,   635,\n",
       "        25289,  3294, 16895,   837,   884,   355,  1642,   262,   983,   517,\n",
       "        43486,   329,  2168, 29661,   764, 15684, 11915,   371,  4548,    64,\n",
       "         8835,    73,   280,   290, 26777,  7286, 13704, 13231, 43354,  1111,\n",
       "         4504,   422,  2180, 12784,   837,  1863,   351,   569, 18354,  7496,\n",
       "        17740,  2873,  3437, 33687,  5303, 18024])"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e70e1c",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Okay so I've got a dataset and a tokenizer. However, I think we need some further changes to the dataset, so that we can use it for training. To apply JEPA here, we need a \"source\" batch of tokens and a \"target\" batch of tokens, where we train the joint representations to be equivalent. \n",
    "\n",
    "I'm thinking a good way to do this to split texts in the tokenizer  `__getitem__` method. This will probably need a min_split_size and max_split_size to avoid splits being overly large or small.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcbafb",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response \n",
    "\n",
    "Is the approach that I'm suggesting above sensible? Is there anything I might be missing? \n",
    "\n",
    "One thing I'm wondering is if having different splits as inputs to the model will have an impact on positional encoding.\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Your intuition about positional encoding is spot on - this is a critical consideration!\n",
    "\n",
    "**The issue**: If you just split a sequence and feed source/target as separate sequences starting from position 0, you lose the positional relationship. The model won't know *where* in the original sequence the target spans were.\n",
    "\n",
    "**How JEPA handles this (vision analogy)**:\n",
    "- Context encoder: visible patches with **original** position embeddings\n",
    "- Target encoder: masked patches with **original** position embeddings  \n",
    "- Predictor: takes context embeddings + **target positions** â†’ predicts target representations\n",
    "\n",
    "**For text, I'd suggest this structure**:\n",
    "\n",
    "```python\n",
    "def __getitem__(self, idx):\n",
    "    tokens = self.chunks[idx]  # Full sequence [seq_len]\n",
    "    \n",
    "    # Generate span mask (True = masked/target)\n",
    "    mask = self.generate_span_mask(len(tokens))\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokens,           # Full sequence\n",
    "        \"target_mask\": mask,           # Which positions to predict\n",
    "        \"positions\": torch.arange(len(tokens))  # Explicit positions\n",
    "    }\n",
    "```\n",
    "\n",
    "Then in your model:\n",
    "- **Context encoder**: processes `input_ids[~target_mask]` with `positions[~target_mask]`\n",
    "- **Target encoder**: processes `input_ids[target_mask]` with `positions[target_mask]`\n",
    "- **Predictor**: uses context output + `positions[target_mask]` to predict target representations\n",
    "\n",
    "**Key insight**: The predictor needs to receive the target positions as input (often via learned position queries or just concatenated position embeddings) so it knows *where* to predict.\n",
    "\n",
    "Does this align with how you were thinking about it, or would you like me to sketch out the span masking logic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4d962",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response\n",
    "\n",
    "Yeah, that makes sense. That being said I don't think we need the positional key here, as that can be generated in the model. \n",
    "\n",
    "Can you start by sketching out the generate_span_mask logic? I think I want this so that it splits into 2 spans at a random index, with a min length to avoid splits that are too small for the model to learn. \n",
    "\n",
    "Try and make this minimal... I essentially just want a boolean mask based on a random index between min_length and seq_len - min_length \n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "```python\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def generate_span_mask(seq_len, min_length=32):\n",
    "    \"\"\"Split sequence into context (False) and target (True) at random index.\"\"\"\n",
    "    split_idx = random.randint(min_length, seq_len - min_length)\n",
    "    mask = torch.zeros(seq_len, dtype=torch.bool)\n",
    "    mask[split_idx:] = True  # Second half is target\n",
    "    return mask\n",
    "```\n",
    "\n",
    "You could also randomly flip which half is target:\n",
    "\n",
    "```python\n",
    "def generate_span_mask(seq_len, min_length=32):\n",
    "    split_idx = random.randint(min_length, seq_len - min_length)\n",
    "    mask = torch.zeros(seq_len, dtype=torch.bool)\n",
    "    mask[split_idx:] = True\n",
    "    if random.random() > 0.5:\n",
    "        mask = ~mask  # Flip so first half is target\n",
    "    return mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e85a4",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response\n",
    "\n",
    "I like version one of this. Can you now integrate this into the dataset logic.\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "```python\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=256, batch_size=10000, min_span_length=32):\n",
    "        self.min_span_length = min_span_length\n",
    "        \n",
    "        # Filter non-empty text\n",
    "        texts = [t for t in dataset[\"text\"] if t.strip()]\n",
    "        \n",
    "        # Tokenize in batches\n",
    "        all_tokens = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            encoded = tokenizer(batch, add_special_tokens=False)[\"input_ids\"]\n",
    "            for ids in encoded:\n",
    "                all_tokens.extend(ids)\n",
    "        \n",
    "        tokens = torch.tensor(all_tokens)\n",
    "        \n",
    "        # Chunk into fixed-length sequences\n",
    "        num_chunks = len(tokens) // max_length\n",
    "        self.chunks = tokens[:num_chunks * max_length].view(num_chunks, max_length)\n",
    "    \n",
    "    def generate_span_mask(self, seq_len):\n",
    "        split_idx = random.randint(self.min_span_length, seq_len - self.min_span_length)\n",
    "        mask = torch.zeros(seq_len, dtype=torch.bool)\n",
    "        mask[split_idx:] = True\n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.chunks[idx]\n",
    "        mask = self.generate_span_mask(len(tokens))\n",
    "        return {\n",
    "            \"input_ids\": tokens,\n",
    "            \"target_mask\": mask,\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205e9f6",
   "metadata": {
    "time_run": "2026-01-20T19:30:33.632112+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 9343\n",
      "Sample: {'input_ids': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198,  2311,\n",
      "           73, 13090,   645,   569, 18354,  7496,   513,  1058,   791, 47398,\n",
      "        17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,   112,\n",
      "         5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,  6578,\n",
      "          764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,   837,\n",
      "         8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,  2354,\n",
      "         2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,  2712,\n",
      "         2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,   329,\n",
      "          262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,  2869,\n",
      "          837,   340,   318,   262,  2368,   983,   287,   262,   569, 18354,\n",
      "         7496,  2168,   764, 12645,   278,   262,   976, 21748,   286, 16106,\n",
      "          290,  1103,  2488,    12,    31,   640, 11327,   355,   663, 27677,\n",
      "          837,   262,  1621,  4539, 10730,   284,   262,   717,   983,   290,\n",
      "         5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,  2422,\n",
      "         4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,  5498,\n",
      "         1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,   389,\n",
      "        46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,    88,\n",
      "        12552,   366,   764,   220,   198,   383,   983,  2540,  2478,   287,\n",
      "         3050,   837,  6872,   625,   257,  1588,  6903,   286,   262,   670,\n",
      "         1760,   319,   569, 18354,  7496, 17740,  2873,   764,  2893,   340,\n",
      "        17383,   262,  3210,  3033,   286,   262,  2168,   837,   340,   635,\n",
      "        25289,  3294, 16895,   837,   884,   355,  1642,   262,   983,   517,\n",
      "        43486,   329,  2168, 29661,   764, 15684, 11915,   371,  4548,    64,\n",
      "         8835,    73,   280,   290, 26777,  7286, 13704, 13231, 43354,  1111,\n",
      "         4504,   422,  2180, 12784,   837,  1863,   351,   569, 18354,  7496,\n",
      "        17740,  2873,  3437, 33687,  5303, 18024]), 'target_mask': tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=256, batch_size=10000, min_span_length=32):\n",
    "        self.min_span_length = min_span_length\n",
    "        \n",
    "        # Filter non-empty text\n",
    "        texts = [t for t in dataset[\"text\"] if t.strip()]\n",
    "        \n",
    "        # Tokenize in batches\n",
    "        all_tokens = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            encoded = tokenizer(batch, add_special_tokens=False)[\"input_ids\"]\n",
    "            for ids in encoded:\n",
    "                all_tokens.extend(ids)\n",
    "        \n",
    "        tokens = torch.tensor(all_tokens)\n",
    "        \n",
    "        # Chunk into fixed-length sequences\n",
    "        num_chunks = len(tokens) // max_length\n",
    "        self.chunks = tokens[:num_chunks * max_length].view(num_chunks, max_length)\n",
    "    \n",
    "    def generate_span_mask(self, seq_len):\n",
    "        split_idx = random.randint(self.min_span_length, seq_len - self.min_span_length)\n",
    "        mask = torch.zeros(seq_len, dtype=torch.bool)\n",
    "        mask[split_idx:] = True\n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.chunks[idx]\n",
    "        mask = self.generate_span_mask(len(tokens))\n",
    "        return {\n",
    "            \"input_ids\": tokens,\n",
    "            \"target_mask\": mask,\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "train_dataset = ChunkedTextDataset(dataset[\"train\"], tokenizer, max_length=256)\n",
    "print(f\"Number of chunks: {len(train_dataset)}\")\n",
    "print(f\"Sample:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815621d",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response\r\n",
    "\r\n",
    "I'm now happy with how this is splitting and chunking.  \r\n",
    "\r\n",
    "Can you create a pytorch lightning data module for processing this data?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class LeJEPATextDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        max_length=256,\n",
    "        min_span_length=32,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.max_length = max_length\n",
    "        self.min_span_length = min_span_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "        \n",
    "        self.train_dataset = ChunkedTextDataset(\n",
    "            dataset[\"train\"], tokenizer, self.max_length, min_span_length=self.min_span_length\n",
    "        )\n",
    "        self.val_dataset = ChunkedTextDataset(\n",
    "            dataset[\"validation\"], tokenizer, self.max_length, min_span_length=self.min_span_length\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "dm = LeJEPATextDataModule(batch_size=32)\n",
    "dm.setup()\n",
    "print(f\"Train batches: {len(dm.train_dataloader())}\")\n",
    "print(f\"Val batches: {len(dm.val_dataloader())}\")\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "print(f\"input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"target_mask shape: {batch['target_mask'].shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901a0ee",
   "metadata": {
    "time_run": "2026-01-20T19:30:35.805406+00:00"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class LeJEPATextDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        max_length=256,\n",
    "        min_span_length=32,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.max_length = max_length\n",
    "        self.min_span_length = min_span_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "        \n",
    "        self.train_dataset = ChunkedTextDataset(\n",
    "            dataset[\"train\"], tokenizer, self.max_length, min_span_length=self.min_span_length\n",
    "        )\n",
    "        self.val_dataset = ChunkedTextDataset(\n",
    "            dataset[\"validation\"], tokenizer, self.max_length, min_span_length=self.min_span_length\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f667318",
   "metadata": {
    "time_run": "2026-01-20T19:30:38.555895+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 292\n",
      "Val batches: 31\n",
      "input_ids shape: torch.Size([32, 256])\n",
      "target_mask shape: torch.Size([32, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm = LeJEPATextDataModule(batch_size=32)\n",
    "dm.setup()\n",
    "print(f\"Train batches: {len(dm.train_dataloader())}\")\n",
    "print(f\"Val batches: {len(dm.val_dataloader())}\")\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "print(f\"input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"target_mask shape: {batch['target_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe88cac",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Next, we need to define the model. There's a few things to work out here:\n",
    "- How to apply SIGReg. \n",
    "- Defining an encoder that can encoder both the source and target spans. \n",
    "- Defining a predictor that can predict the target encoder's representations of masked spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712532e4",
   "metadata": {},
   "source": [
    "## SIGReg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7c5c4",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response \n",
    "\n",
    "To start, I want to work out how to implement SIGReg. Can you implement this, referencing both [the paper](https://arxiv.org/abs/2511.08544) and [this implementation on github](https://github.com/galilai-group/lejepa). If easier I'm happy for you to use the github implementation directly.\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://github.com/galilai-group/lejepa)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01JvPDFMtq6kKnbg8ei3JZ1r\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://github.com/galilai-group/lejepa\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'# galilai-group/lejepa\\\\n\\\\n\\\\n## / Files\\\\n- LICENSE\\\\n- MINIMAL.md\\\\n- README.md\\\\n- \\ud83d\\udcc1 eval\\\\n- \\ud83d\\udcc1 figures\\\\n- \\ud83d\\udcc1 lejepa\\\\n- pyproject.toml\\\\n- \\ud83d\\udcc1 scripts\\\\n- setup.py\\\\n- \\ud83d\\udcc1 tests\\\\n\\\\n## README\\\\n# LeJEPA\\\\n**Lean Joint-Embedding Predictive Architecture (LeJEPA): Provable and Scalable Self-Supervised Learning Without the Heuristics**\\\\n[GitHub Repository](https://github.com/rbalestr-lab/lejepa)  \\\\n[arXiv:2511.08544](https://arxiv.org/abs/2511.08544)\\\\n---\\\\n\\\\nRush to our [minimal working example](MINIMAL.md) to see a full-fledge working example (ViT, inet).\\\\n\\\\n## Demo\\\\n\\\\n<img src=\\\"eval/output1.gif\\\" controls widt<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://github.com/galilai-group/lejepa/tree/main/â€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01QMF2DitEf7Q3dKfLskhjh6\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://github.com/galilai-group/lejepa/tree/main/lejepa\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'# galilai-group/lejepa\\\\n\\\\n\\\\n## /lejepa Files\\\\n- __init__.py\\\\n- \\ud83d\\udcc1 __pycache__\\\\n- \\ud83d\\udcc1 multivariate\\\\n- \\ud83d\\udcc1 univariate'\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/galilai-group/leâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01GAx4ckTnfjWD7PUrfBtxwq\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/galilai-group/lejepa/main/lejepa/__init__.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'from . import univariate\\\\nfrom . import multivariate\\\\n\\\\n__all__ = [\\\"univariate\\\", \\\"multivariate\\\"]\\\\n'\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/galilai-group/leâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01JhuonZnZtXiXAxVRoNTnpk\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/galilai-group/lejepa/main/lejepa/multivariate/__init__.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'from .slicing import SlicingUnivariateTest\\\\nfrom .bhep import BHEP\\\\nfrom .bhep_m import BHEP_M\\\\nfrom .comb import COMB\\\\nfrom .hz import HZ\\\\nfrom .hv import HV\\\\n\\\\n__all__ = [SlicingUnivariateTest, BHEP, BHEP_M, COMB, HZ, HV]\\\\n'\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/galilai-group/leâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01Y2geh4fdasWQbs9gvHY6CL\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/galilai-group/lejepa/main/lejepa/univariate/__init__.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'from .anderson_darling import AndersonDarling\\\\nfrom .cramer_von_mises import CramerVonMises\\\\nfrom .entropy import Entropy\\\\nfrom .shapiro_wilk import ShapiroWilk\\\\nfrom .watson import Watson\\\\nfrom .moments import Moments\\\\nfrom .likelihood import NLL\\\\nfrom .jarque_bera import ExtendedJarqueBera, VCReg\\\\nfrom .epps_pulley import EppsPulley, EppsPulley\\\\nfrom .base import UnivariateTest\\\\n\\\\n__all__ = [\\\\n    AndersonDarling,\\\\n    CramerVonMises,\\\\n    Entropy,\\\\n    ShapiroWilk,\\\\n    Watson,\\\\n    NLL,\\\\n    ExtendedJarqueBera,\\\\n    VCReg,\\\\n    EppsPulley,\\\\n    EppsPulley,\\\\n    UnivariateTest,\\\\n]\\\\n'\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/galilai-group/leâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01QoPGfnbfuJgeNZW73th4Xi\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/galilai-group/lejepa/main/lejepa/multivariate/slicing.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'import torch\\\\nfrom torch import distributed as dist\\\\nfrom torch.distributed._functional_collectives import (\\\\n    all_reduce as functional_all_reduce,\\\\n)\\\\n\\\\n\\\\ndef all_reduce(x, op=\\\"AVG\\\"):\\\\n    if dist.is_available() and dist.is_initialized():\\\\n        return functional_all_reduce(x, op.lower(), dist.group.WORLD)\\\\n    else:\\\\n        return x\\\\n\\\\n\\\\nclass SlicingUnivariateTest(torch.nn.Module):\\\\n    \\\"\\\"\\\"\\\\n    Multivariate distribution test using random slicing and univariate test statistics.\\\\n    This module extends univariate statistical tests to multivariate data by projecting\\\\n    samples onto <TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/galilai-group/leâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01229WWk1815a1NHpf76bRAY\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/galilai-group/lejepa/main/lejepa/univariate/epps_pulley.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'import torch\\\\nfrom .base import UnivariateTest\\\\nfrom torch import distributed as dist\\\\nfrom torch.distributed.nn import all_reduce as functional_all_reduce\\\\nfrom torch.distributed.nn import ReduceOp\\\\n\\\\n\\\\ndef all_reduce(x, op=\\\"AVG\\\"):\\\\n    if dist.is_available() and dist.is_initialized():\\\\n        op = ReduceOp.__dict__[op.upper()]\\\\n        return functional_all_reduce(x, op)\\\\n    else:\\\\n        return x\\\\n\\\\n\\\\nclass EppsPulley(UnivariateTest):\\\\n    \\\"\\\"\\\"\\\\n    Fast Epps-Pulley two-sample test statistic for univariate distributions.\\\\n\\\\n    This implementation uses numerical integration over the char<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "I've got the key implementation. The SIGReg loss in LeJEPA works by:\n",
    "1. **Slicing**: Project high-dimensional embeddings onto random 1D directions\n",
    "2. **Univariate testing**: Apply a statistical test (like Epps-Pulley) to check if projections are standard normal\n",
    "3. **Aggregate**: Average the test statistics across all slices\n",
    "\n",
    "Here's a simplified implementation you can use directly:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EppsPulley(nn.Module):\n",
    "    \"\"\"Epps-Pulley test for univariate normality via characteristic function.\"\"\"\n",
    "    \n",
    "    def __init__(self, t_max=3.0, num_points=17):\n",
    "        super().__init__()\n",
    "        assert num_points % 2 == 1\n",
    "        t = torch.linspace(0, t_max, num_points)\n",
    "        dt = t_max / (num_points - 1)\n",
    "        weights = torch.full((num_points,), 2 * dt)\n",
    "        weights[[0, -1]] = dt\n",
    "        phi = torch.exp(-0.5 * t ** 2)\n",
    "        \n",
    "        self.register_buffer(\"t\", t)\n",
    "        self.register_buffer(\"phi\", phi)\n",
    "        self.register_buffer(\"weights\", weights * phi)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (*, N, K) where N=samples, K=slices\n",
    "        N = x.size(-2)\n",
    "        x_t = x.unsqueeze(-1) * self.t  # (*, N, K, num_points)\n",
    "        \n",
    "        cos_mean = torch.cos(x_t).mean(-3)  # (*, K, num_points)\n",
    "        sin_mean = torch.sin(x_t).mean(-3)\n",
    "        \n",
    "        err = (cos_mean - self.phi).square() + sin_mean.square()\n",
    "        return (err @ self.weights) * N  # (*, K)\n",
    "\n",
    "\n",
    "class SIGReg(nn.Module):\n",
    "    \"\"\"Sketched Isotropic Gaussian Regularization loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_slices=1024, t_max=3.0, num_points=17):\n",
    "        super().__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.univariate_test = EppsPulley(t_max, num_points)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (batch_size, embed_dim)\n",
    "        # Standardize\n",
    "        embeddings = (embeddings - embeddings.mean(0)) / (embeddings.std(0) + 1e-6)\n",
    "        \n",
    "        # Random projections\n",
    "        D = embeddings.size(-1)\n",
    "        A = torch.randn(D, self.num_slices, device=embeddings.device)\n",
    "        A = A / A.norm(dim=0, keepdim=True)  # Normalize to unit vectors\n",
    "        \n",
    "        projected = embeddings @ A  # (batch_size, num_slices)\n",
    "        \n",
    "        # Apply univariate test and average\n",
    "        stats = self.univariate_test(projected)  # (num_slices,)\n",
    "        return stats.mean()\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "sigreg = SIGReg(num_slices=1024)\n",
    "embeddings = torch.randn(256, 768)  # batch of embeddings\n",
    "loss = sigreg(embeddings)\n",
    "print(f\"SIGReg loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "The key insight: SIGReg encourages embeddings to be **isotropic Gaussian** distributed. If the embeddings perfectly match N(0, I), the loss is zero. This prevents collapse without needing stop-gradient or EMA tricks â€” though for JEPA you'll still use EMA for the target encoder to provide stable prediction targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9612a",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.139093+00:00"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EppsPulley(nn.Module):\n",
    "    \"\"\"Epps-Pulley test for univariate normality via characteristic function.\"\"\"\n",
    "    \n",
    "    def __init__(self, t_max=3.0, num_points=17):\n",
    "        super().__init__()\n",
    "        assert num_points % 2 == 1\n",
    "        t = torch.linspace(0, t_max, num_points)\n",
    "        dt = t_max / (num_points - 1)\n",
    "        weights = torch.full((num_points,), 2 * dt)\n",
    "        weights[[0, -1]] = dt\n",
    "        phi = torch.exp(-0.5 * t ** 2)\n",
    "        \n",
    "        self.register_buffer(\"t\", t)\n",
    "        self.register_buffer(\"phi\", phi)\n",
    "        self.register_buffer(\"weights\", weights * phi)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (*, N, K) where N=samples, K=slices\n",
    "        N = x.size(-2)\n",
    "        x_t = x.unsqueeze(-1) * self.t  # (*, N, K, num_points)\n",
    "        \n",
    "        cos_mean = torch.cos(x_t).mean(-3)  # (*, K, num_points)\n",
    "        sin_mean = torch.sin(x_t).mean(-3)\n",
    "        \n",
    "        err = (cos_mean - self.phi).square() + sin_mean.square()\n",
    "        return (err @ self.weights) * N  # (*, K)\n",
    "\n",
    "\n",
    "class SIGReg(nn.Module):\n",
    "    \"\"\"Sketched Isotropic Gaussian Regularization loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_slices=1024, t_max=3.0, num_points=17):\n",
    "        super().__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.univariate_test = EppsPulley(t_max, num_points)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (batch_size, embed_dim)\n",
    "        # Standardize\n",
    "        embeddings = (embeddings - embeddings.mean(0)) / (embeddings.std(0) + 1e-6)\n",
    "        \n",
    "        # Random projections\n",
    "        D = embeddings.size(-1)\n",
    "        A = torch.randn(D, self.num_slices, device=embeddings.device)\n",
    "        A = A / A.norm(dim=0, keepdim=True)  # Normalize to unit vectors\n",
    "        \n",
    "        projected = embeddings @ A  # (batch_size, num_slices)\n",
    "        \n",
    "        # Apply univariate test and average\n",
    "        stats = self.univariate_test(projected)  # (num_slices,)\n",
    "        return stats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b5c5e",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.144935+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGReg loss: 0.5811\n"
     ]
    }
   ],
   "source": [
    "sigreg = SIGReg(num_slices=1024)\n",
    "embeddings = torch.randn(256, 768)  # batch of embeddings\n",
    "loss = sigreg(embeddings)\n",
    "print(f\"SIGReg loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df270cc",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcfa80e",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response\n",
    "\n",
    "Thanks. Now, how can we build the encoder model. I want a transformer based text encoder that can get representations of the source and targets, with masking based on the source/target split. We'll also need a predictor that predicts the target from the context?\n",
    "\n",
    "I initially was under the impression that the target encoder would be a copy of the source encoder that is updated as a moving average. However it appears from re-reading the LeJEPA paper that SIGReg supposedly removes the need for EMA. I can also see that other and other JEPA implementations, [e.g. this repo](https://github.com/jerber/lang-jepa/blob/main/src/encoder/train.py), aren't using stop gradients. I'm going to go ahead using the same encoder for source and target, however I will check for collapse (i.e. encoder producing constant vectors), as this is known to be an issue with JEPA architectures.  \n",
    "\n",
    "Because I'm working in an environment with limited compute, I think it would be better for the predictor just to be a linear layer, rather than a transformer. \n",
    "\n",
    "Lets do this bit by bit, starting with creating an encoder and showing how we can get source and target representations in a batched way. I think the challenge here is that batches of sources and targets will be inconsistent lengths. If possible I think we should deal with this through passing the mask when we get representations.  \n",
    "\n",
    "Can you use the code here https://github.com/jerber/lang-jepa/blob/main/src/encoder/models.py and here https://github.com/jerber/lang-jepa/blob/main/src/encoder/train.py as inspiration, particularly how the masking is applied. I'm happy for the positional encoding to be a simple embedding layer. \n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/jerber/lang-jepaâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01GeQxv4TmpEVg9dA2tdq4dY\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/jerber/lang-jepa/main/src/encoder/models.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'import torch\\\\nimport torch.nn as nn\\\\nimport torch.nn.functional as F\\\\nfrom torch import Tensor\\\\nfrom transformers import AutoConfig, AutoModel\\\\n\\\\nfrom src.common.config import LANGJEPAConfig\\\\n\\\\n\\\\nclass TextTransformer(nn.Module):\\\\n    \\\"\\\"\\\"Text encoder based on pre-trained transformer models.\\\"\\\"\\\"\\\\n\\\\n    def __init__(self, config: LANGJEPAConfig):\\\\n        super().__init__()\\\\n        # Load base model config and update with our settings\\\\n        model_config = AutoConfig.from_pretrained(config.data.tokenizer_path)\\\\n        model_config.update(\\\\n            {\\\\n                \\\"hidden_size\\\": config<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>read_url(url=https://raw.githubusercontent.com/jerber/lang-jepaâ€¦)</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01NzJPyix64rpADsCFQLPwHb\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://raw.githubusercontent.com/jerber/lang-jepa/main/src/encoder/train.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'import os\\\\nimport time\\\\nfrom pathlib import Path\\\\n\\\\nimport torch\\\\nimport torch.nn.functional as F\\\\nfrom dotenv import load_dotenv\\\\nfrom torch.utils.data import DataLoader\\\\n\\\\nimport wandb\\\\nfrom src.common.config import LANGJEPAConfig\\\\nfrom src.common.datasets.fineweb_edu import TextDataset, worker_init_fn\\\\nfrom src.common.logging import AverageMeter, CSVLogger\\\\nfrom src.encoder.collator import Batch, Collator\\\\nfrom src.encoder.models import TextPredictor, TextTransformer\\\\nfrom src.encoder.utils.helper import init_optimizer, load_checkpoint, save_checkpoint\\\\nfrom src.encoder.utils.monitor impor<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "I see - the lang-jepa repo uses `torch.no_grad()` on the **target** encoder (same weights, just no gradient flow) and passes attention masks to handle variable lengths. Here's a simplified encoder setup for your use case:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Simple transformer encoder for text.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=4, max_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len)\n",
    "            attention_mask: (batch, seq_len) - 1 for valid tokens, 0 for masked\n",
    "        Returns:\n",
    "            (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        B, L = input_ids.shape\n",
    "        positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
    "        \n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Convert attention_mask to transformer format (True = ignore)\n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    def get_representations(self, input_ids, target_mask, pool=\"mean\"):\n",
    "        \"\"\"\n",
    "        Get separate context and target representations.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: (batch, seq_len)\n",
    "            target_mask: (batch, seq_len) - True for target positions\n",
    "            pool: how to aggregate token representations\n",
    "        \"\"\"\n",
    "        # Get full sequence representations\n",
    "        full_repr = self.forward(input_ids)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        context_mask = ~target_mask  # True for context positions\n",
    "        \n",
    "        if pool == \"mean\":\n",
    "            # Masked mean pooling for context\n",
    "            context_repr = (full_repr * context_mask.unsqueeze(-1)).sum(dim=1)\n",
    "            context_repr = context_repr / context_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "            \n",
    "            # Masked mean pooling for target\n",
    "            target_repr = (full_repr * target_mask.unsqueeze(-1)).sum(dim=1)\n",
    "            target_repr = target_repr / target_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling: {pool}\")\n",
    "        \n",
    "        return context_repr, target_repr\n",
    "\n",
    "\n",
    "class LinearPredictor(nn.Module):\n",
    "    \"\"\"Simple linear predictor: context repr -> predicted target repr.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, context_repr):\n",
    "        return self.proj(context_repr)\n",
    "```\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "# Initialize\n",
    "vocab_size = 50257  # GPT-2 vocab size\n",
    "encoder = TextEncoder(vocab_size, embed_dim=256, num_heads=4, num_layers=4)\n",
    "predictor = LinearPredictor(embed_dim=256)\n",
    "\n",
    "# Forward pass with a batch\n",
    "batch = train_dataset[0]\n",
    "input_ids = batch[\"input_ids\"].unsqueeze(0)  # Add batch dim\n",
    "target_mask = batch[\"target_mask\"].unsqueeze(0)\n",
    "\n",
    "context_repr, target_repr = encoder.get_representations(input_ids, target_mask)\n",
    "predicted_repr = predictor(context_repr)\n",
    "\n",
    "print(f\"Context repr: {context_repr.shape}\")\n",
    "print(f\"Target repr: {target_repr.shape}\")\n",
    "print(f\"Predicted repr: {predicted_repr.shape}\")\n",
    "```\n",
    "\n",
    "Key design choices:\n",
    "- **Single encoder** processes full sequence, then we mask-pool to get context/target representations\n",
    "- This avoids variable-length batching complexity\n",
    "- During training, `target_repr` is computed with `torch.no_grad()` (like in lang-jepa) to prevent collapse\n",
    "- The **predictor is just a linear layer** with LayerNorm as you requested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebeb3e1",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response\r\n",
    "\r\n",
    "The mask here is unused in the encoder here which I'm finding a bit confusing. Can you confirm if a TransformerEncoder in pytorch can see ahead if there is no mask... i.e. can value in position 10 see the value in position 11? \n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Yes, **TransformerEncoder is bidirectional by default** - position 10 can absolutely see position 11 (and all other positions). It uses full self-attention with no causal masking.\n",
    "\n",
    "The `src_key_padding_mask` parameter is only for **padding** - telling the model which positions are padding tokens to ignore. Since your sequences are fixed-length chunks (no padding), the mask is indeed unused.\n",
    "\n",
    "This is actually **correct for JEPA** - you want bidirectional attention because:\n",
    "- Context tokens should attend to all other context tokens\n",
    "- Target tokens should attend to all other target tokens\n",
    "- You're learning representations, not doing autoregressive generation\n",
    "\n",
    "If you wanted causal (can't see ahead) attention, you'd need to pass a causal `mask` (not `src_key_padding_mask`):\n",
    "\n",
    "```python\n",
    "# Causal mask (if you wanted it - you don't for JEPA)\n",
    "causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len)\n",
    "x = self.transformer(x, mask=causal_mask)\n",
    "```\n",
    "\n",
    "So the current setup is correct - the encoder sees all positions bidirectionally, which is what you want for learning JEPA-style representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb316861",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "### prompt / response\r\n",
    "\r\n",
    "But in this context you pass both the source and target through the encoder without a mask, so surely the context values will be able to see the target values and visa versa. I can see in https://github.com/jerber/lang-jepa/blob/main/src/encoder/train.py on line 136 and 144/145 that the source and target are masked, but in your version no masking is applied\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "You're absolutely right - I made an error. The current code lets context tokens attend to target tokens during self-attention, which leaks information.\n",
    "\n",
    "The fix is to apply the mask **during** the encoder forward pass:\n",
    "\n",
    "```python\n",
    "def get_context_target_representations(context_encoder, target_encoder, input_ids, target_mask):\n",
    "    \"\"\"\n",
    "    Get pooled context and target representations with proper masking.\n",
    "    \n",
    "    Context encoder only sees context positions.\n",
    "    Target encoder only sees target positions.\n",
    "    \"\"\"\n",
    "    context_mask = ~target_mask  # True for context positions\n",
    "    \n",
    "    # Context encoder: mask out target positions during attention\n",
    "    context_hidden = context_encoder(input_ids, attention_mask=context_mask)\n",
    "    context_hidden = context_hidden * context_mask.unsqueeze(-1).float()\n",
    "    context_repr = context_hidden.sum(dim=1) / context_mask.sum(dim=1, keepdim=True).float()\n",
    "    \n",
    "    # Target encoder: mask out context positions during attention\n",
    "    with torch.no_grad():\n",
    "        target_hidden = target_encoder(input_ids, attention_mask=target_mask)\n",
    "        target_hidden = target_hidden * target_mask.unsqueeze(-1).float()\n",
    "        target_repr = target_hidden.sum(dim=1) / target_mask.sum(dim=1, keepdim=True).float()\n",
    "    \n",
    "    return context_repr, target_repr\n",
    "```\n",
    "\n",
    "Now:\n",
    "- `context_encoder` gets `attention_mask=context_mask` â†’ can only attend to context positions\n",
    "- `target_encoder` gets `attention_mask=target_mask` â†’ can only attend to target positions\n",
    "\n",
    "The `src_key_padding_mask` in the encoder forward pass will prevent attention to the masked-out positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f8c3a",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "## Full Class\r\n",
    "\r\n",
    "### prompt / response\r\n",
    "\r\n",
    "Okay, I think I'm happy with this now. Can you now combine these previous steps into a full LightningModule?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Simple transformer encoder for text.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=4, max_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, L = input_ids.shape\n",
    "        positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
    "        \n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class LinearPredictor(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class LeJEPAText(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,\n",
    "        embed_dim=256,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        max_length=512,\n",
    "        dropout=0.1,\n",
    "        lr=1e-4,\n",
    "        sigreg_weight=0.1,\n",
    "        num_slices=1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Shared encoder for context and target\n",
    "        self.encoder = TextEncoder(\n",
    "            vocab_size, embed_dim, num_heads, num_layers, max_length, dropout\n",
    "        )\n",
    "        self.predictor = LinearPredictor(embed_dim)\n",
    "        self.sigreg = SIGReg(num_slices=num_slices)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.sigreg_weight = sigreg_weight\n",
    "    \n",
    "    def get_representations(self, input_ids, target_mask):\n",
    "        \"\"\"Get context and target representations with proper masking.\"\"\"\n",
    "        context_mask = ~target_mask\n",
    "        \n",
    "        # Context: only attend to context positions\n",
    "        context_hidden = self.encoder(input_ids, attention_mask=context_mask)\n",
    "        context_hidden = context_hidden * context_mask.unsqueeze(-1).float()\n",
    "        context_repr = context_hidden.sum(dim=1) / context_mask.sum(dim=1, keepdim=True).float()\n",
    "        \n",
    "        # Target: only attend to target positions (no gradient)\n",
    "        with torch.no_grad():\n",
    "            target_hidden = self.encoder(input_ids, attention_mask=target_mask)\n",
    "            target_hidden = target_hidden * target_mask.unsqueeze(-1).float()\n",
    "            target_repr = target_hidden.sum(dim=1) / target_mask.sum(dim=1, keepdim=True).float()\n",
    "        \n",
    "        return context_repr, target_repr\n",
    "    \n",
    "    def forward(self, input_ids, target_mask):\n",
    "        context_repr, target_repr = self.get_representations(input_ids, target_mask)\n",
    "        predicted_repr = self.predictor(context_repr)\n",
    "        return predicted_repr, target_repr, context_repr\n",
    "    \n",
    "    def compute_loss(self, batch):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        target_mask = batch[\"target_mask\"]\n",
    "        \n",
    "        predicted_repr, target_repr, context_repr = self(input_ids, target_mask)\n",
    "        \n",
    "        # MSE loss between predicted and actual target representations\n",
    "        prediction_loss = nn.functional.mse_loss(predicted_repr, target_repr)\n",
    "        \n",
    "        # SIGReg on context representations to prevent collapse\n",
    "        sigreg_loss = self.sigreg(context_repr)\n",
    "        \n",
    "        total_loss = prediction_loss + self.sigreg_weight * sigreg_loss\n",
    "        \n",
    "        return total_loss, prediction_loss, sigreg_loss, context_repr\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, pred_loss, sigreg_loss, context_repr = self.compute_loss(batch)\n",
    "        \n",
    "        # Monitor for collapse\n",
    "        repr_std = context_repr.std(dim=0).mean()\n",
    "        \n",
    "        self.log(\"train/loss\", total_loss, prog_bar=True)\n",
    "        self.log(\"train/pred_loss\", pred_loss)\n",
    "        self.log(\"train/sigreg_loss\", sigreg_loss)\n",
    "        self.log(\"train/repr_std\", repr_std, prog_bar=True)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        total_loss, pred_loss, sigreg_loss, context_repr = self.compute_loss(batch)\n",
    "        \n",
    "        repr_std = context_repr.std(dim=0).mean()\n",
    "        \n",
    "        self.log(\"val/loss\", total_loss, prog_bar=True)\n",
    "        self.log(\"val/pred_loss\", pred_loss)\n",
    "        self.log(\"val/sigreg_loss\", sigreg_loss)\n",
    "        self.log(\"val/repr_std\", repr_std)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "```\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "model = LeJEPAText(vocab_size=50257, embed_dim=256, num_heads=4, num_layers=4)\n",
    "\n",
    "# Test with a batch\n",
    "batch = next(iter(dm.train_dataloader()))\n",
    "loss = model.training_step(batch, 0)\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "Key features:\n",
    "- **`repr_std`** logged to monitor collapse (if it drops near 0, representations are collapsing)\n",
    "- **`torch.no_grad()`** on target to prevent trivial solutions\n",
    "- **SIGReg** on context representations for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8790aa",
   "metadata": {},
   "source": [
    "## Initiate and Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9ccc5",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.187123+00:00"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Simple transformer encoder for text.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=4, max_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, L = input_ids.shape\n",
    "        positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
    "        \n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class LinearPredictor(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class LeJEPAText(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,\n",
    "        embed_dim=256,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        max_length=512,\n",
    "        dropout=0.1,\n",
    "        lr=1e-4,\n",
    "        sigreg_weight=0.1,\n",
    "        num_slices=1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Shared encoder for context and target\n",
    "        self.encoder = TextEncoder(\n",
    "            vocab_size, embed_dim, num_heads, num_layers, max_length, dropout\n",
    "        )\n",
    "        self.predictor = LinearPredictor(embed_dim)\n",
    "        self.sigreg = SIGReg(num_slices=num_slices)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.sigreg_weight = sigreg_weight\n",
    "    \n",
    "    def get_representations(self, input_ids, target_mask):\n",
    "        \"\"\"Get context and target representations with proper masking.\"\"\"\n",
    "        context_mask = ~target_mask\n",
    "        \n",
    "        # Context: only attend to context positions\n",
    "        context_hidden = self.encoder(input_ids, attention_mask=context_mask)\n",
    "        context_hidden = context_hidden * context_mask.unsqueeze(-1).float()\n",
    "        context_repr = context_hidden.sum(dim=1) / context_mask.sum(dim=1, keepdim=True).float()\n",
    "        \n",
    "        # Target: only attend to target positions (no gradient)\n",
    "        with torch.no_grad():\n",
    "            target_hidden = self.encoder(input_ids, attention_mask=target_mask)\n",
    "            target_hidden = target_hidden * target_mask.unsqueeze(-1).float()\n",
    "            target_repr = target_hidden.sum(dim=1) / target_mask.sum(dim=1, keepdim=True).float()\n",
    "        \n",
    "        return context_repr, target_repr\n",
    "    \n",
    "    def forward(self, input_ids, target_mask):\n",
    "        context_repr, target_repr = self.get_representations(input_ids, target_mask)\n",
    "        predicted_repr = self.predictor(context_repr)\n",
    "        return predicted_repr, target_repr, context_repr\n",
    "    \n",
    "    def compute_loss(self, batch):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        target_mask = batch[\"target_mask\"]\n",
    "        \n",
    "        predicted_repr, target_repr, context_repr = self(input_ids, target_mask)\n",
    "        \n",
    "        # MSE loss between predicted and actual target representations\n",
    "        prediction_loss = nn.functional.mse_loss(predicted_repr, target_repr)\n",
    "        \n",
    "        # SIGReg on context representations to prevent collapse\n",
    "        sigreg_loss = self.sigreg(context_repr)\n",
    "        \n",
    "        total_loss = prediction_loss + self.sigreg_weight * sigreg_loss\n",
    "        \n",
    "        return total_loss, prediction_loss, sigreg_loss, context_repr\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, pred_loss, sigreg_loss, context_repr = self.compute_loss(batch)\n",
    "        \n",
    "        # Monitor for collapse\n",
    "        repr_std = context_repr.std(dim=0).mean()\n",
    "        \n",
    "        self.log(\"train/loss\", total_loss, prog_bar=True)\n",
    "        self.log(\"train/pred_loss\", pred_loss)\n",
    "        self.log(\"train/sigreg_loss\", sigreg_loss)\n",
    "        self.log(\"train/repr_std\", repr_std, prog_bar=True)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        total_loss, pred_loss, sigreg_loss, context_repr = self.compute_loss(batch)\n",
    "        \n",
    "        repr_std = context_repr.std(dim=0).mean()\n",
    "        \n",
    "        self.log(\"val/loss\", total_loss, prog_bar=True)\n",
    "        self.log(\"val/pred_loss\", pred_loss)\n",
    "        self.log(\"val/sigreg_loss\", sigreg_loss)\n",
    "        self.log(\"val/repr_std\", repr_std)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97176c1c",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.200980+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 3353600\n"
     ]
    }
   ],
   "source": [
    "model = LeJEPAText(vocab_size=50257, embed_dim=64, num_heads=2, num_layers=2)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"Number of Parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b3694",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.227656+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.3 s, sys: 844 ms, total: 9.15 s\n",
      "Wall time: 385 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5128, -0.0883,  0.1644,  ...,  0.2210,  0.1130,  0.1568],\n",
       "         [-0.5430, -0.2594, -0.1270,  ...,  0.1448, -0.0542,  0.1635],\n",
       "         [-0.3083,  0.1464, -0.0937,  ...,  0.0654,  0.2852, -0.1501],\n",
       "         ...,\n",
       "         [-0.4473, -0.1915, -0.1485,  ...,  0.2285, -0.2723, -0.1203],\n",
       "         [-0.4323, -0.1273,  0.0560,  ...,  0.1686, -0.0838,  0.1050],\n",
       "         [-0.4209,  0.0393, -0.0346,  ...,  0.1213, -0.0704,  0.1748]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[-0.4988, -0.0431,  0.3527,  ...,  0.1180,  0.1283,  0.0995],\n",
       "         [-0.4808, -0.0066, -0.0075,  ...,  0.1063,  0.0564,  0.1553],\n",
       "         [-0.3580, -0.0674,  0.0278,  ...,  0.1353,  0.0917,  0.1066],\n",
       "         ...,\n",
       "         [-0.4294,  0.0062, -0.1615,  ...,  0.3190, -0.1362,  0.2172],\n",
       "         [-0.4899, -0.2853, -0.0208,  ...,  0.0021,  0.0751,  0.3868],\n",
       "         [-0.5488,  0.0648,  0.0065,  ..., -0.0263,  0.0421,  0.1304]]))"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "context_repr, target_repr = model.get_representations(**batch)\n",
    "context_repr, target_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349b588",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.626564+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.5 ms, sys: 0 ns, total: 38.5 ms\n",
      "Wall time: 1.06 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5477, -0.7017, -0.2800,  ...,  1.4106, -0.6975, -0.2430],\n",
       "        [-0.1500, -1.1685,  0.4107,  ...,  0.1386, -1.0401,  0.6272],\n",
       "        [ 1.0390, -1.0849, -0.7849,  ...,  0.4333, -0.6909,  0.9622],\n",
       "        ...,\n",
       "        [-0.3853, -1.1286,  0.0256,  ..., -0.0637, -0.9749,  0.4740],\n",
       "        [ 0.5579, -1.2967, -0.4796,  ...,  0.5714, -0.5915,  0.4055],\n",
       "        [ 0.4296, -0.5910, -0.1715,  ...,  0.2881, -1.1545,  0.4743]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predicted_repr = model.predictor(context_repr)\n",
    "predicted_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2b01c",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.631104+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 489 Î¼s, sys: 20 Î¼s, total: 509 Î¼s\n",
      "Wall time: 348 Î¼s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1227, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prediction_loss = nn.functional.mse_loss(predicted_repr, target_repr)\n",
    "prediction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96a795",
   "metadata": {
    "time_run": "2026-01-20T19:30:41.637934+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 ms, sys: 0 ns, total: 38.1 ms\n",
      "Wall time: 2.25 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8375, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sigreg_loss = model.sigreg(context_repr)\n",
    "sigreg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9dec89",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "## Train\r\n",
    "\r\n",
    "Now, lets use a lightning trainer and fit the model\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "```python\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "# Initialize data and model\n",
    "dm = LeJEPATextDataModule(batch_size=32, max_length=256, min_span_length=32)\n",
    "model = LeJEPAText(vocab_size=50257, embed_dim=64, num_heads=2, num_layers=2, lr=1e-4)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        ModelCheckpoint(monitor=\"val/loss\", mode=\"min\", save_top_k=1),\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, dm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d28c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 0/1  <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> 0/292 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:00 â€¢ -:--:--</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">0.00it/s</span> <span style=\"font-style: italic\">v_num: 5.000 val/loss: 1.134</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 0/1  \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 0/292 \u001b[2m0:00:00 â€¢ -:--:--\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: 5.000 val/loss: 1.134\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "# Initialize data and model\n",
    "dm = LeJEPATextDataModule(batch_size=32, max_length=256, min_span_length=32)\n",
    "model = LeJEPAText(vocab_size=50257, embed_dim=64, num_heads=2, num_layers=2, max_length=256, lr=1e-4)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        ModelCheckpoint(monitor=\"val/loss\", mode=\"min\", save_top_k=1),\n",
    "    ],\n",
    "    log_every_n_steps=50,\n",
    "    val_check_interval=50\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d116ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeJEPAText.load_from_checkpoint(\"lightning_logs/version_4/checkpoints/epoch=1-step=542.ckpt\")\n",
    "\n",
    "representation = model.encoder(batch[\"input_ids\"]).mean(dim=1)\n",
    "representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation.mean(dim=1), representation.std(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2701610",
   "metadata": {
    "time_run": "2026-02-11T09:59:28.001978+00:00"
   },
   "source": [
    "# Conclude\n",
    "\n",
    "Above we have a minimal implementation of LeJEPA on text. This has definately helped with my understanding of how JEPA architectures work, and I think could provide a good foundation for further implementations of LeJEPA for text domains. \n",
    "\n",
    "There would be quite a few things I would want to do next, including:\n",
    "\n",
    "- Scale up the model and dataset. \n",
    "- Add proper logging and training monitoring. \n",
    "- Downstream evaluations on text classification datasets.  \n",
    "- Add a decoder and use this for text generation.\n",
    "\n",
    "Building this has been a pain. A couple of key issues:\n",
    "\n",
    "- The LLM doesn't understand JEPA well and has made basic mistakes in the implementation. Most of the prompts above have come from multiple revisions, and my own rewrites after double checking other implementations and the LeJEPA paper. I suppose because it's a relatively new technique that it isn't widely seen in the training data. \n",
    "- solveit keeps crashing as I'm running. I think this is just because compute is limited and it's running out of memory. In the future for these kinds of tasks I'll want to use a GPU, which will either involve working locally or working out to SSH into a cluster while in solveit (potential follow up post!). \n",
    "\n",
    "As a next step to this, I'm keep to implement this more comprehensively. I've relied quite a lot on [this repo](https://github.com/jerber/lang-jepa), and it could be interesting to try and add LeJEPA in here, while potentially also adding evaluation so that we can compare LeJEPA to the repos original implementation, and with other text encoders. "
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "standard",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
