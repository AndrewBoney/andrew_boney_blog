<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andrew Boney">
<meta name="dcterms.date" content="2026-01-28">
<meta name="description" content="Reading notes and research regarding GPTRec">

<title>GPTRec Paper Read – Andrew Boney Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-f24a88a7144a9a90ed386dce2ccc5eba.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Andrew Boney Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/andrewboney"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/barry_bones_"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">GPTRec Paper Read</h1>
                  <div>
        <div class="description">
          Reading notes and research regarding <a href="https://arxiv.org/abs/2306.11114">GPTRec</a>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">research-paper</div>
                <div class="quarto-category">RecSys</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andrew Boney </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 28, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gptrec" id="toc-gptrec" class="nav-link active" data-scroll-target="#gptrec">GPTRec</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">1 - Introduction</a></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work">2 - Related Work</a>
  <ul class="collapse">
  <li><a href="#adaptations-of-language-models-for-sequential-recommender-systems" id="toc-adaptations-of-language-models-for-sequential-recommender-systems" class="nav-link" data-scroll-target="#adaptations-of-language-models-for-sequential-recommender-systems">2.1 - Adaptations of Language Models for Sequential Recommender Systems</a></li>
  <li><a href="#recommendations-as-text-generation" id="toc-recommendations-as-text-generation" class="nav-link" data-scroll-target="#recommendations-as-text-generation">2.2 - Recommendations as Text Generation</a></li>
  </ul></li>
  <li><a href="#top-k-and-next-k-recommendations" id="toc-top-k-and-next-k-recommendations" class="nav-link" data-scroll-target="#top-k-and-next-k-recommendations">3 - Top-K and Next-K Recommendations</a></li>
  <li><a href="#gptrec-1" id="toc-gptrec-1" class="nav-link" data-scroll-target="#gptrec-1">4 - GPTRec</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">4.1 Architecture</a></li>
  <li><a href="#tokenisation" id="toc-tokenisation" class="nav-link" data-scroll-target="#tokenisation">4.2 Tokenisation</a></li>
  <li><a href="#training-objective-and-loss-function" id="toc-training-objective-and-loss-function" class="nav-link" data-scroll-target="#training-objective-and-loss-function">4.3 Training Objective and Loss Function</a></li>
  <li><a href="#generating-recommendations" id="toc-generating-recommendations" class="nav-link" data-scroll-target="#generating-recommendations">4.4 Generating Recommendations…</a></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">6 - Experiments</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a>
  <ul class="collapse">
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further Reading</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<!--
I'm using this notebook for a blog, where both prompts and responses will be rendered in quarto, with this notebook itself serving as the blog post. I want to define my own headings in the blog, but still use your responses, so when you respond ensure there are no `#`s to avoid them being rendered as headings (instead mark sections you want to demarkate as bold)
-->
<section id="gptrec" class="level1">
<h1>GPTRec</h1>
<p>In this post, I’ll be going through the paper <a href="https://arxiv.org/abs/2306.11114">“Generative Sequential Recommendation with GPTRec”</a>. This was released in 2023 by Aleksandr V. Petrov and Craig Macdonald from the University of Glasgow, who have done extensive work in the sequential recommendation space. GPTRec builds on previous approaches such as <a href="https://arxiv.org/abs/1808.09781">SASRec</a>, which uses a self-attentive architecture for next-item prediction, and <a href="https://arxiv.org/abs/1904.06690">BERT4Rec</a>, which applies BERT-style bidirectional encoders to recommendation problems.</p>
<p>I first came across their work in an <a href="https://open.spotify.com/episode/5DLy3bzJE1MX6v9uvrPo29">episode of the Recsperts podcast</a>, where they discuss their research. Their paper is also mentioned in the <a href="https://arxiv.org/abs/2404.00579">Gen-RecSys survey</a>, which I recently <a href="https://andrewboney.github.io/andrew_boney_blog/posts/Gen-RecSys%20Paper%20Read">wrote a post about</a>.</p>
<p>This excerpt from the abstract summarises the approach:</p>
<blockquote class="blockquote">
<p>“This paper presents the GPTRec sequential recommendation model, which is based on the GPT-2 architecture. GPTRec can address large vocabulary issues by splitting item ids into sub-id tokens using a novel SVD Tokenisation algorithm based on quantised item embeddings from a SVD decomposition of the user-item interaction matrix. The paper also presents a novel Next-K recommendation strategy, which generates recommendations item-by-item, considering already recommended items.”</p>
</blockquote>
<p>So beyond applying transformers to recommendations, the authors aim to address multiple practical challenges in the field.</p>
<p>I find this approach very interesting. It feels like a natural progression for recommender architectures, incorporating best practices from broader AI developments over the past few years. I’ve previously implemented transformer-based encoders in industry, and while those attempts successfully improved model accuracy, I felt there was still significant room for optimisation.</p>
</section>
<section id="introduction" class="level1">
<h1>1 - Introduction</h1>
<p><strong>Sequential vs Standard Recommendation</strong></p>
<p>The authors frame sequential recommendation as distinct from traditional approaches—rather than treating user-item interactions as an unordered set, sequential models leverage the temporal ordering of interactions. This matters when consumption patterns are inherently sequential (e.g., watching a film series in order, or purchasing complementary items over time). The task is cast as next-item prediction: given a user’s interaction history, predict what they’ll engage with next.</p>
<p><strong>The Top-K Problem and Next-K Solution</strong></p>
<p>A key insight is that existing models like SASRec and BERT4Rec use a “score-and-rank” approach where each item is scored independently. This creates issues when similar items receive similar scores, leading to redundant recommendations (the “coffee beans problem”—if you bought a coffee machine, you might see ten different coffee bean varieties dominating your recommendations). The authors propose Next-K generation, where recommendations are produced item-by-item, with each position aware of what’s already been recommended.</p>
<p><strong>Scaling to Large Catalogues</strong></p>
<p>The second challenge is memory: with 10M items and 256-dimensional embeddings, the embedding table alone requires &gt;10GB of GPU memory. The authors propose SVD Tokenisation—decomposing item IDs into sub-tokens derived from quantised SVD embeddings of the user-item matrix. This decouples vocabulary size from memory, similar to how BPE tokenisation handles large text vocabularies.</p>
<p><strong>Architecture Distinction</strong></p>
<p>GPTRec uses the GPT-2 decoder architecture with Cross-Entropy loss, contrasting with SASRec’s Binary Cross-Entropy. The authors claim that this loss function choice alone improves performance (+35% NDCG@10 over SASRec on MovieLens-1M).</p>
</section>
<section id="related-work" class="level1">
<h1>2 - Related Work</h1>
<section id="adaptations-of-language-models-for-sequential-recommender-systems" class="level2">
<h2 class="anchored" data-anchor-id="adaptations-of-language-models-for-sequential-recommender-systems">2.1 - Adaptations of Language Models for Sequential Recommender Systems</h2>
<p>The paper draws a parallel between NLP and sequential recommendation—both domains work with ordered sequences (words in text, items in interaction history). This structural similarity has driven researchers to adapt NLP architectures for recommendation.</p>
<p>Key models discussed:</p>
<ul>
<li><a href="https://arxiv.org/abs/1511.06939"><strong>GRU4Rec</strong></a> — One of the first neural sequential recommenders, adapted from GRU (originally designed for machine translation)</li>
<li><a href="https://github.com/kang205/SASRec"><strong>SASRec</strong></a> — Uses the Transformer decoder, shifting input sequences one element left (i.e., predict the next item given history).</li>
<li><a href="https://arxiv.org/abs/1904.06690"><strong>BERT4Rec</strong></a> — Uses the Transformer encoder, predicting randomly masked items from sequences. This differs somewhat from a “predict the next item” task.</li>
</ul>
<p>The authors note that while these models achieve strong ranking metrics (NDCG@K), they share two fundamental limitations: (1) the Top-K scoring strategy is inflexible for optimising auxiliary objectives like diversity or serendipity, and (2) GPU memory requirements become prohibitive as catalogue size grows.</p>
</section>
<section id="recommendations-as-text-generation" class="level2">
<h2 class="anchored" data-anchor-id="recommendations-as-text-generation">2.2 - Recommendations as Text Generation</h2>
<p>Recent work explores using pre-trained LLMs directly for recommendation:</p>
<ul>
<li><a href="https://arxiv.org/abs/2203.13366">P5</a> — Generates item and user IDs as text strings via prompting</li>
<li><a href="https://arxiv.org/abs/2205.08084">M6-Rec</a> — Generates item titles directly as recommendations</li>
<li><a href="https://arxiv.org/abs/2305.05065">TIGER</a> — Introduces “semantic IDs” where items are represented as token sets derived from side information (e.g., product category)</li>
</ul>
<p>The authors explicitly distinguish their approach from this line of work. Pre-trained models encode world knowledge about the recommendation domain, making it difficult to isolate what the model learns from interaction patterns versus what it brings from pre-training. GPTRec deliberately avoids side information to better understand “the properties of the generative approach” in a classical sequential setting where only interaction sequences are available.</p>
</section>
</section>
<section id="top-k-and-next-k-recommendations" class="level1">
<h1>3 - Top-K and Next-K Recommendations</h1>
<p>Typical recommendation models use a top-k method, where they filter and rank the highest scoring candidates, generally either based on similarity between user and item embeddings, or a scoring model. The issue here is that recommendations aren’t interdependant - high scoring items are likely to be from the same product category, but in practice you don’t want to recommend users variations of the same thing (this issue is called diversity). There is also issues around complimentaryness (find a better word for this) and serendipidy (i.e.&nbsp;recommend things that are new to them).</p>
<p>This paper introduces Next-K as an alternative, wherby recommendations are generated iteratively based on user / item queries <em>as well as items already recommended</em>. This can improve diversity, although at higher compute cost and training complexity.</p>
</section>
<section id="gptrec-1" class="level1">
<h1>4 - GPTRec</h1>
<section id="architecture" class="level2">
<h2 class="anchored" data-anchor-id="architecture">4.1 Architecture</h2>
<p>GPTRec’s architecture is based on GPT-2, inheriting its decoder-only Transformer design with a few modifications: layer normalisation is moved to the beginning of each transformer block (pre-LN), residual weights use a modified initialisation scheme, and positional encodings are learned rather than using fixed sinusoidal patterns.</p>
<p>The core idea mirrors how language models treat words—here, item IDs are treated as tokens. A user’s interaction history becomes a sequence of tokens fed into the model, which learns to predict the next token (i.e., the next item the user will interact with).</p>
<p>The challenge emerges at scale. When your catalogue contains millions of items, each requiring its own embedding vector, memory requirements become prohibitive. A 10M item catalogue with 256-dimensional embeddings would need over 10GB just for the embedding table—before accounting for gradients, model weights, or intermediate computations. To address this, GPTRec introduces sub-item tokenisation, decomposing each item into multiple smaller tokens.</p>
</section>
<section id="tokenisation" class="level2">
<h2 class="anchored" data-anchor-id="tokenisation">4.2 Tokenisation</h2>
<p>The paper proposes SVD Tokenisation as a method to compress the item vocabulary. The algorithm works as follows:</p>
<ul>
<li><strong>Build the interaction matrix</strong> — Construct a user-item interaction matrix M where entries indicate whether a user interacted with an item</li>
<li><strong>SVD decomposition</strong> — Perform truncated SVD: M ≈ U × Σ × E^T, where E contains the item embeddings with t latent components</li>
<li><strong>Normalise</strong> — Scale each embedding dimension to the [0, 1] range and add small Gaussian noise (mean=0, std=10⁻⁵) to ensure no two items have identical embeddings</li>
<li><strong>Quantise</strong> — Discretise each dimension into v bins, converting continuous embeddings into discrete token IDs</li>
<li><strong>Offset</strong> — Shift the i-th dimension’s values by v×(i-1) so each dimension has its own token range (dimension 0 uses tokens [0, v-1], dimension 1 uses [v, 2v-1], etc.)</li>
</ul>
<p>The result is that each item is now represented by t sub-tokens, where each sub-token comes from a vocabulary of size v. The total embedding table size becomes t×v rather than scaling with the number of items.</p>
<p>For large catalogues, e.g.&nbsp;LaftFM-1b dataset with ~33m items, this reduces the size of the embedding table &gt; 99%, depending on selection of parameters <code>v</code> i.e.&nbsp;number of tokens per dimension, and <code>t</code> i.e.&nbsp;number of dimensions.</p>
<p>The paper gives psudocode for this, which I’ve emulated in python below.</p>
<div id="425defa0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> csr_matrix</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse.linalg <span class="im">import</span> svds</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> svd_tokenize_items(interaction_matrix, n_components<span class="op">=</span><span class="dv">8</span>, n_bins<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    SVD Tokenisation as described in GPTRec Section 4.2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        interaction_matrix: User-item interaction matrix (users x items)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        n_components: Number of latent dimensions (= number of sub-tokens per item)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        n_bins: Number of discrete bins per dimension (vocabulary size per sub-token)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        item_tokens: Array of shape (n_items, n_components) with sub-token ids</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        bin_edges: List of bin edges for each dimension (for decoding)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to sparse if needed</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(interaction_matrix, csr_matrix):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        interaction_matrix <span class="op">=</span> csr_matrix(interaction_matrix)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># SVD decomposition - get item embeddings (V^T from U * S * V^T)</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We want item factors, so we transpose to get items in rows</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    U, S, Vt <span class="op">=</span> svds(interaction_matrix, k<span class="op">=</span>n_components)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Item embeddings: V^T transposed and scaled by singular values</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    item_embeddings <span class="op">=</span> Vt.T <span class="op">*</span> S  <span class="co"># Shape: (n_items, n_components)</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Quantise each dimension into n_bins discrete tokens</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    item_tokens <span class="op">=</span> np.zeros_like(item_embeddings, dtype<span class="op">=</span>np.int32)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    bin_edges <span class="op">=</span> []</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> dim <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create bin edges based on percentiles for balanced bins</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        edges <span class="op">=</span> np.percentile(item_embeddings[:, dim], </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                              np.linspace(<span class="dv">0</span>, <span class="dv">100</span>, n_bins <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        bin_edges.append(edges)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assign each item's value to a bin (token)</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        item_tokens[:, dim] <span class="op">=</span> np.clip(</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            np.digitize(item_embeddings[:, dim], edges[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>]), </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>, n_bins <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> item_tokens, bin_edges</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dummy interaction matrix (100 users, 500 items)</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>n_users, n_items <span class="op">=</span> <span class="dv">100</span>, <span class="dv">500</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>interactions <span class="op">=</span> (np.random.rand(n_users, n_items) <span class="op">&lt;</span> <span class="fl">0.1</span>).astype(<span class="bu">float</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize items</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">4</span>  <span class="co"># 4 sub-tokens per item</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>n_bins <span class="op">=</span> <span class="dv">16</span>       <span class="co"># 16 possible values per sub-token</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>item_tokens, bin_edges <span class="op">=</span> svd_tokenize_items(</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    interactions, n_components<span class="op">=</span>n_components, n_bins<span class="op">=</span>n_bins</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"og items shape:"</span>, np.arange(n_items).shape)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"new items shape:"</span>, item_tokens.shape)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"og items num embeds:"</span>, n_items)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"new items num embeds:"</span>, <span class="bu">len</span>(np.unique(item_tokens.flatten())))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>og items shape: (500,)
new items shape: (500, 4)
og items num embeds: 500
new items num embeds: 16</code></pre>
</div>
</div>
<p>The memory savings can be dramatic. Table 1 in the paper shows that for the LastFM-1b dataset (~32M items), using t=8 tokens with v=2048 bins reduces embedding storage to just 0.05% of the original size—from over 30GB down to 16MB.</p>
<p>That said, this technique comes with trade-offs:</p>
<ul>
<li><strong>Added complexity</strong> — The embedding creation pipeline becomes more involved, requiring SVD computation and careful quantisation</li>
<li><strong>Hyperparameter tuning</strong> — You need to select appropriate values for v (bins per dimension) and t (number of dimensions), which may vary by dataset</li>
<li><strong>Information loss</strong> — Quantisation inherently groups similar items together, which may aid generalisation but sacrifices item-level specificity</li>
</ul>
</section>
<section id="training-objective-and-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="training-objective-and-loss-function">4.3 Training Objective and Loss Function</h2>
<p>GPTRec follows the standard language modelling objective with Cross-Entropy loss. The model learns to predict each token in the sequence given all preceding tokens, factorising the sequence probability as:</p>
<p>p(S) = ∏ p(sᵢ | s₁, s₂, …, sᵢ₋₁)</p>
<p>Like GPT-2, the model uses causal masking to generate predictions for all positions in a single forward pass—each position predicts the next token, enabling efficient training over entire sequences.</p>
<p>One aspect I find interesting is how the sequence structure works with sub-item tokenisation. Given a user’s history, the input sequence becomes a mix of sub-item tokens (which have no natural ordering within an item) interleaved with the temporal ordering of actual purchases. This creates an unusual sequence structure that the positional encoder must handle. The authors’ choice of learned positional encodings over sinusoidal ones likely helps the model adapt to this hybrid ordering.</p>
</section>
<section id="generating-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="generating-recommendations">4.4 Generating Recommendations…</h2>
<p>GPTRec supports two recommendation strategies: Top-K (the standard approach) and Next-K (their proposed alternative).</p>
<p>Top-K generation works as expected in one-token-per-item mode—the model outputs a probability distribution over items, and you select the K highest-scoring items. This is typical of how recommendation systems generate item predictions.</p>
<p>Next-K generation produces recommendations iteratively. At each step, the model considers the user’s history plus all items already recommended, then selects the next best item. This is equivalent to greedy decoding in language models and naturally avoids recommending the same item twice.</p>
<p><strong>The multi-token generation challenge</strong></p>
<p>When using sub-item tokenisation, generating recommendations becomes more involved since each item is represented by multiple tokens. The paper addresses this in Section 4.4.2 with a sampling-based approach:</p>
<ol type="1">
<li>Use standard autoregressive generation to sample t tokens, forming a candidate item representation</li>
<li>Check if the generated token sequence maps to a valid item ID (discarding invalid sequences)</li>
<li>Score valid candidates using the chain rule, multiplying the conditional probabilities of each sub-token</li>
<li>Apply standard Top-K ranking over the candidates, treating any item that wasn’t sampled as having a score of negative infinity</li>
</ol>
<p>The authors acknowledge this is somewhat inefficient, noting they generate K=50 candidates just to produce 10 recommendations, since many sampled sequences are either invalid or duplicates.</p>
<p>While the mechanism is clear, there are some practical limitations worth considering:</p>
<ul>
<li><strong>Sampling coverage:</strong> Since only sampled candidates are considered, you could miss relevant items that simply weren’t generated during sampling. The model’s recommendations are bounded by what it happens to sample.</li>
<li><strong>Quantisation collisions:</strong> Items that share identical token sequences (a natural consequence of the quantisation step) become indistinguishable to the model.</li>
<li><strong>Scoring assumptions:</strong> Using the chain rule to combine sub-token probabilities into an item score is a reasonable heuristic, but it’s not obvious this is optimal. The model wasn’t explicitly trained to produce well-calibrated item-level probabilities through this mechanism.</li>
</ul>
<p><strong>On Next-K alignment</strong></p>
<p>The paper acknowledges that Next-K generation isn’t well-aligned with the training objective. The model is trained to predict the actual next item a user interacted with, but at inference time it’s asked to predict item i+1 as if the user had interacted with items 1 through i—which they haven’t. The authors suggest reinforcement learning techniques (similar to InstructGPT) as future work to better align training with generation.</p>
<p>I’m somewhat sceptical of the Next-K approach in general. While the motivation of solving the diverity issue found in Top-K recommendations is sound, the solution feels awkward. You’re essentially recommending subsequent items under the assumption that the user will engage with each recommendation in sequence, which rarely reflects real user behaviour. The computational overhead (K forward passes instead of one) compounds this concern. There are likely better approaches to the diversity problem that don’t require such strong assumptions about sequential engagement. That said, instruction tuning could help align the model’s training objective with its generation objective, potentially making Next-K more viable.</p>
</section>
</section>
<section id="experiments" class="level1">
<h1>6 - Experiments</h1>
<p>The authors evaluate GPTRec on the MovieLens-1M dataset using a leave-one-out strategy—holding out each user’s most recent interaction for testing, with a small validation set (128 users’ second-to-last interactions) for early stopping.</p>
<p><strong>Experimental setup notes:</strong></p>
<p>The configuration choices are reasonable but raise some questions. The maximum sequence length is capped at 100 tokens, yet the dataset’s average sequence length is 165 interactions. This truncation means the model only sees the most recent portion of longer user histories, potentially discarding useful signal.</p>
<p>The authors also acknowledge a known issue with MovieLens-1M: timestamps reflect when users <em>rated</em> films, not when they <em>watched</em> them. This introduces noise into the sequential ordering that the model relies on. I suspect this penalises autoregressive models (GPTRec, SASRec) more heavily than BERT4Rec, since bidirectional models with random masking are less dependent on strict temporal ordering.</p>
<p><strong>RQ1: One-token-per-item performance</strong></p>
<p>Comparing GPTRec-TopK against baselines, the model achieves NDCG@10 of 0.146—a 35% improvement over SASRec (0.108) but 4% below BERT4Rec (0.152). The authors attribute SASRec’s underperformance to its use of Binary Cross-Entropy loss versus GPTRec’s standard Cross-Entropy, suggesting the loss function choice alone accounts for much of the gap.</p>
<p><strong>RQ2: Multi-token-per-item trade-offs</strong></p>
<p>Performance degrades when using SVD tokenisation, though the model remains competitive with SASRec. With 4 tokens per item and 512 bins, GPTRec matches SASRec’s NDCG@10 (0.108) while requiring 40% fewer embeddings. However, this falls well short of GPTRec in one-token mode (0.146).</p>
<p>The trend is clear: more bins per token improves quality. With only 128 bins, NDCG@10 drops to 0.091; increasing to 2048 bins recovers performance to 0.124. This suggests the quantisation granularity is a key bottleneck—coarser discretisation loses too much item-level information.</p>
<p><strong>RQ3: Next-K degradation</strong></p>
<p>At K=1, Top-K and Next-K strategies are equivalent (both simply select the highest-probability item). As K increases, Next-K quality degrades, reaching 75% of Top-K performance at K=10. This aligns with the authors’ acknowledgment that the training objective isn’t aligned with iterative generation—the model learns to predict what users <em>actually</em> interacted with next, not what would be a good <em>recommendation</em> given hypothetical prior recommendations.</p>
<p>Despite this, GPTRec-NextK still matches SASRec’s performance (both at 0.108 NDCG@10), which the authors frame as a viable starting point for future work with reinforcement learning fine-tuning.</p>
<p><img src="../../assets/gptrec_perf.png" class="img-fluid"></p>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>The authors outline several directions for future work:</p>
<ul>
<li><strong>Larger datasets</strong> — The experiments use only MovieLens-1M (~3.4K items). The memory benefits of SVD tokenisation become most compelling at scale (Table 1 shows &gt;99% reduction for LastFM-1b’s 32M items), but these gains remain untested.</li>
<li><strong>Beam search for multi-token generation</strong> — The current sampling approach discards many invalid sequences and may miss relevant items. Beam search could improve candidate coverage.</li>
<li><strong>RL fine-tuning for Next-K</strong> — The training objective (predict the actual next item) is misaligned with Next-K generation (predict good recommendations given hypothetical prior recommendations). The authors suggest InstructGPT-style reinforcement learning to close this gap.</li>
<li><strong>Hyperparameter exploration</strong> — The paper tests limited configurations; broader tuning of model depth, embedding dimensions, and tokenisation parameters could yield improvements.</li>
</ul>
<p><strong>My thoughts</strong></p>
<p>The core ideas here are compelling: applying generative LM techniques to recommendation, addressing vocabulary scaling through sub-item tokenisation, and rethinking the independence assumption in Top-K ranking. However, the empirical results are underwhelming.</p>
<p>GPTRec-TopK underperforms BERT4Rec by 4% on NDCG@10, and the multi-token mode further degrades to match SASRec—the very baseline the authors critique. The Next-K strategy, while conceptually interesting for diversity, performs 25% worse than Top-K at the standard K=10 cutoff and relies on assumptions about sequential user engagement that rarely hold in practice.</p>
<p>The tokenisation approach also leaves practical questions unanswered. When multiple items map to identical token sequences (an inevitable consequence of quantisation), how should the system distinguish between them at serving time? The paper doesn’t address this collision problem or how to recover specific item IDs from generated sub-tokens.</p>
<p>I appreciate this as foundational work exploring generative approaches to recommendation, but the gap between the compelling motivation and the actual results is substantial. The promised benefits—memory efficiency, flexible objectives, interdependent recommendations—remain largely theoretical until demonstrated on large-scale catalogues with competitive accuracy.</p>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<p><strong>Core Sequential Recommendation Models</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1808.09781">SASRec: Self-Attentive Sequential Recommendation</a> — Kang &amp; McAuley (2018). Decoder-only Transformer for next-item prediction using causal attention.</li>
<li><a href="https://arxiv.org/abs/1904.06690">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</a> — Sun et al.&nbsp;(2019). Applies BERT-style masked language modelling to recommendation.</li>
<li><a href="https://arxiv.org/abs/1511.06939">GRU4Rec: Session-based Recommendations with Recurrent Neural Networks</a> — Hidasi et al.&nbsp;(2016). One of the first neural sequential recommenders, using GRU architecture.</li>
</ul>
<p><strong>LLM-based Recommendation</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2203.13366">P5: Recommendation as Language Processing</a> — Geng et al.&nbsp;(2022). Unified pretrain-prompt-predict paradigm for recommendation.</li>
<li><a href="https://arxiv.org/abs/2205.08084">M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</a> — Cui et al.&nbsp;(2022). Generates item titles directly as recommendations.</li>
<li><a href="https://arxiv.org/abs/2305.05065">TIGER: Recommender Systems with Generative Retrieval</a> — Rajput et al.&nbsp;(2023). Introduces semantic IDs derived from item side information.</li>
</ul>
<p><strong>Foundational Architectures</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> — Vaswani et al.&nbsp;(2017). The original Transformer paper.</li>
<li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2: Language Models are Unsupervised Multitask Learners</a> — Radford et al.&nbsp;(2019). The architecture GPTRec is based on.</li>
</ul>
<p><strong>Surveys &amp; Meta-analyses</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2404.00579">A Survey on Large Language Models for Recommendation</a> — The Gen-RecSys survey mentioned in the post.</li>
<li><a href="https://arxiv.org/abs/2207.07483">A Systematic Review and Replicability Study of BERT4Rec</a> — Petrov &amp; Macdonald (2022). By the same authors; provides baselines used in GPTRec experiments.</li>
</ul>
<p><strong>Related Techniques</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2203.02155">InstructGPT: Training Language Models to Follow Instructions with Human Feedback</a> — Ouyang et al.&nbsp;(2022). RL fine-tuning approach suggested for Next-K alignment.</li>
<li><a href="https://dl.acm.org/doi/10.1145/290941.291025">MMR: The Use of Diversity-Based Reranking</a> — Carbonell &amp; Goldstein (1998). Classic diversification method that Next-K aims to replace.</li>
</ul>
<div id="740506c0" class="cell" data-time_run="2026-02-04T14:25:26.155619+00:00">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cd <span class="op">~/</span>andrew_boney_blog <span class="op">&amp;&amp;</span> quarto preview <span class="op">--</span>port <span class="dv">8000</span> <span class="op">--</span>host <span class="fl">0.0.0.0</span> <span class="op">--</span>no<span class="op">-</span>browser</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-blue-fg ansi-bold">Preparing to preview</span>

<span class="ansi-blue-fg ansi-bold">
[1/4] posts/GPTRec Paper Read/index.ipynb</span>

<span class="ansi-blue-fg ansi-bold">
[2/4] posts/Gen-RecSys Paper Read/index.ipynb</span>

<span class="ansi-blue-fg ansi-bold">
[3/4] posts/LeJEPA Paper Read/index.ipynb</span>

<span class="ansi-blue-fg ansi-bold">
[4/4] index.qmd</span>



<span class="ansi-green-fg">Watching files for changes</span>

<span class="ansi-green-fg">Browse at </span><span style="text-decoration:underline" class="ansi-green-fg">http://0.0.0.0:8000/posts/Gen-RecSys Paper Read/index.html</span>

Listening on http://0.0.0.0:8000/ (http://localhost:8000/)

<span class="ansi-green-fg">GET: /</span>

<span class="ansi-green-fg">GET: /posts/GPTRec Paper Read/</span>

<span class="ansi-blue-fg ansi-bold">
[1/2] posts/GPTRec Paper Read/index.ipynb</span>

<span class="ansi-blue-fg ansi-bold">
[2/2] index.qmd</span>



<span class="ansi-green-fg">GET: /posts/GPTRec Paper Read/</span>
</pre>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/andrewboney\.github\.io\/andrew_boney_blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>